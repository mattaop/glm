---
output:
  pdf_document: 
    latex_engine: xelatex
  html_document: default
---

--- 
title: "TMA4315: Compulsory exercise 1 (title)" 
subtitle: "Group 0: Name1, Name2 (subtitle)" 
date: "`r format(Sys.time(), '%d.%m.%Y')`" # the current date, can be regular text as well
output: # 3rd letter intentation hierarchy
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
  # pdf_document:
  #  toc: false
  #  toc_depth: 2

---

```{r setup, include = FALSE}
library(formatR)
showsol <- FALSE
library(knitr)
opts_chunk$set(tidy.opts = list(width.cutoff = 68), tidy = TRUE, warning = FALSE, error = FALSE, message = FALSE, echo = TRUE)
```

# Part 1

**Bold**

_italic_

To get a pdf file, make comments of the lines with the "html_document" information, and make the lines with the "pdf_document" information regular, and vice versa.

## a)

Your answer for part 1a)

```{r, include=FALSE}
library(car)
data(SLID, package = "carData")
SLID <- SLID[complete.cases(SLID), ]

```

```{r}
# some R code for part 1a)
library(GGally)
ggpairs(SLID, lower = list(combo = wrap(ggally_facethist, binwidth = 0.5)))
```

From the top row we can see that the wage variable shows a noticeable correlation with education, age and sex. Language on the other hand doesn't seem to have a large impact on the wages. One can see that people with high level (20) of education are distributed over the whole span of wages, but low educated people are centred around low wages, with very few or none at high level of wages.

The age correlates to wages in that there are people from all age categories that have low wages, but that people are more likely to have a higher wage around age 40. Visually, it seems that the average wage decreases with every age bracket above 40, such that those aged 60 have a lower average wage than 50-year-olds, and so forth. The correlation is posivite and substantial, so the overall trend is that higher age correlates to higher wages. There is a correlation between age and education, in that the education level decreases as the age increases. It is a known fact that the average education level have increased over the last 50 years, which correlates with the data set. 

As for the sex variable, males have a somewhat higher median wage, and the first and third wage quartiles in males are respectively higher than those in females. The wage outliers among males also tend to earn more than those in females, suggesting that that the upper few percents of earners will tend to be male.

First we assume that there is a linear relationship between the covariates, ie. the relationship can be expressed as:


$$\bf{Y} = \bf{X} \bf{\beta} + \bf{\varepsilon}$$

In order to perform a multiple linear regression analysis, we must make the following assumptions:
First, the response, which is wage, is a linear combination of the covariates, and errors are additive onto the linear combination, i.e. $\bf{Y} = \bf{X} \bf{\beta} + \bf{\varepsilon}$. Second, that we have homoscedastic error variance and uncorrelated errors, i.e. Cov(ε)=$σ^2\bf{I}$, Cov($ε_i,ε_j$)=0. For this model, this means that along age, sex, language and education, the variance in wage is the same for any observation, or set of observations.

In order for the model to also be "normal", 
.

Additivity of errors: Y=Xβ+ε

Additionally, 
Assumption of normality: ε∼N(0,σ2I)

# Part 2

**a)**

```{r}
# R code for part 2a)
library(mylm)
model1 <- mylm(wages ~ education, data = SLID)
print(model1)
model1b <- lm(wages ~ education, data = SLID)
print(model1b)
```
**b)**
Here is a print out of tha covariance matrix defined as:
$$\Sigma = E\Big[(X-E[X])(X-E[X])^T\Big] = \frac{1}{n}\Big(\sum_{i=1}^{n}(Y_i - \hat Y_i)\Big)(X^TX)^{-1}$$
```{r}
# R code for part 2b
print.default(model1$covariance_matrix)
```
```{r}
# R code for part 2b
summary(model1)
summary(model1b)
```
The intercept estimate as shown in the print out is 4.97169 and the estimated standard error is 0.53429. For the regression coefficient the estimate is 0.79231 and the estimated standard error is 0.03906. Using a Z-test, we get:
$$ P(Z \leq |z|)=2 \cdot \Phi(-|Z|), \hspace{1cm} Z=\frac{x-\mu}{\sigma} $$
In our case, the $H_0$ hypotheses is that $\mu$ is zeros, and thus we get $Z=x/ \sigma$:
```{r}
cat("Z-values for the regression coefficients: ")
print.default(model1$z_value)
```
Computing the p-values using
$$P(Z \leq z) = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{z} e^{-t^2}dt$$
The compution is implemented in our mylm package, and gives the values:
```{r}
cat("P-values for the regression coefficients: ")
print.default(model1$p_value2)
```
Which suggests that both the intercept and the regression  coefficients are significant. The usual level to determen if a coefficient is siginficant or not is a 95%-confidence interval where $P(Z \leq z) < 0.05$. If true, the parameter in question is significant at a 5%-level, which both our parameters are in this case.
**c)**
```{r}
# R code for part 2c)
library(ggplot2)
plot(model1)
plot(model1b, which = c(1))
```
The plot shows the residuals plotted against the fitted values. A residual plot shows if the linear regression is appropriate for the data. A random distribution around the horisontal-axis suggest that there is no systematical error in the regression, and that a linear regression is appropriate. On the other side, if the residuals follow a systematix distribution around the horisontal-axis, there is likely that the relationship between the covariates and the response is non-linear. In this plot we 
interprets the plot as randomly distributed, and that the relationship can be described as a linear regression.

**d)**

* The residual sum of squares (SSE) for this model is computed as: $SSE = \sum_{i=1}^{n} \epsilon_i^2$ where $\epsilon_i = (I-H)Y$. 
```{r, echo = FALSE}
cat("SSE: ")
print.default(model1$sse)
```
In this case The degrees of freedom for this model is the number of dimesions that are free, which can be expressed by $df = n-p-1$, where n is the number of datapoints (in this caes 3987) and p is the number of explanatory parameters (in this case 1). Thus $df = 3985$.

* The total sum of squares (SST) is $\sum_{i=1}^{n}(Y - mean(Y))^2$
```{r, echo = FALSE}
cat("SST: ")
print.default(model1$sst)
```
**e)**
The coefficient of determination $R^2$ is computed $R^2 = SSR/SST = 1 - SSE/SST$, where SSR/SST can be interpreted as how much of the total variability in the data (SST) is described by the regression (SSR). We want the regression to describe the variability in the data, and thus a $R^2$-value as close to 1 as possible is desired ($0 \leq R^2 \leq 1$). In a simple linear regression, $R^2$ is the squared correlation coefficient between the response and the predictor (in this case wage and education*$\hat \beta$), and for multiple linear regression $R^2$ is the squared correlation coefficient between the response and the predicted response. The value for our model is: 
```{r, echo = FALSE}
cat("R-squared: ")
print.default(model1$R_squared)
```
This value is not very good, and suggests that education alone is not a very good predictor for the wage, and that it doesn't desctibe the variability in the data.


# Part 3

**a)**
```{r}
# R code for part 3a)
library(mylm)
model2 <- mylm(wages ~ education + age, data = SLID)
print(model2)
```

**b)**
```{r}
# R code for part 3b)
summary(model2)
```

**c)**
```{r}
# R code for part 3c)
model2a <- mylm(wages ~ education, data = SLID)
model2b <- mylm(wages ~ age, data = SLID)
summary(model2a)
summary(model2b)
```

# Part 4
```{r}
library(mylm)
model3 <- mylm(wages ~ sex + age + language + I(education^2), data = SLID)
model3b <-  lm(wages ~ sex + age + language + I(education^2), data = SLID)
print(model3)
print(model3b)
```


```{r}
library(mylm)
model4 <- mylm(wages ~ language + age + language*age, data = SLID)
model4b <-  lm(wages ~ language + age + language*age, data = SLID)
print(model4)
print(model4b)
```

```{r}
library(mylm)
model5 <- mylm(wages ~ education - 1, data = SLID)
model5b <-  lm(wages ~ education - 1, data = SLID)
print(model5)
print(model5b)
```
