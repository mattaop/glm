---
output:
  pdf_document: default
  html_document: default
---

'--- 
title: "TMA4315: Compulsory exercise 1 (title)" 
subtitle: "Group 0: Name1, Name2 (subtitle)" 
date: "`r format(Sys.time(), '%d.%m.%Y')`" # the current date, can be regular text as well
output: # 3rd letter intentation hierarchy
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
  # pdf_document:
  #  toc: false
  #  toc_depth: 2

---

```{r setup, include = FALSE}
library(formatR)
showsol <- FALSE
library(knitr)
opts_chunk$set(tidy.opts = list(width.cutoff = 68), tidy = TRUE, warning = FALSE, error = FALSE, message = FALSE, echo = TRUE)
```

# Part 1

**Bold**

_italic_

To get a pdf file, make comments of the lines with the "html_document" information, and make the lines with the "pdf_document" information regular, and vice versa.

## a)

Your answer for part 1a)

```{r, include=FALSE}
install.packages("car")
library(car)
data(SLID, package = "carData")
SLID <- SLID[complete.cases(SLID), ]

```

```{r}
# some R code for part 1a)
library(GGally)
ggpairs(SLID, lower = list(combo = wrap(ggally_facethist, binwidth = 0.5)))
```

From the top row we can see that there is a noticeable corrolation between wages, and the education, age and sex. Language on the other hand doesn't seem to have a large impact on the wages. One can see that people with high level (20) of education are distributed over the whole span of wages, but low educated people are centred around low wages, with very few or none at high level of wages.

The age corrolates to wages in that there is people from all age categories that have low wages, but the there is more likley to have a higher wage around age 40, and decreasing when younger or older.


There is a corrolation between age and education, in that the education level decreases as the age increases. There is a known fact that the average education level have increased over the last 50 years, which corrolates with the data set. 


First we assume that there is a linear relationship between the covariates, ie. the relationship can be expressed as:
$$\bf{Y} = \bf{X} \bf{\beta} + \bf{\varepsilon}$$

Linearity of covariates: Y=Xβ+ε. Problem: non-linear relationship?

Homoscedastic error variance: Cov(ε)=σ2I. Problem: Non-constant variance of error terms

Uncorrelated errors: Cov(εi,εj)=0.

Additivity of errors: Y=Xβ+ε

Assumption of normality: ε∼Nn(0,σ2I)

# Part 2

**a)**

```{r}
# some R code for part 2a)
library(mylm)
model1 <- mylm(wages ~ education, data = SLID)
print(model1)
model1b <- lm(wages ~ education, data = SLID)
print(model1b)

```
**b)**
Here is a print out of tha covariance matrix defined as:
$$\Sigma = E\Big[(X-E[X])(X-E[X])^T\Big] = \frac{1}{n}\Big(\sum_{i=1}^{n}(Y_i - \hat Y_i)\Big)(X^TX)^{-1}$$
```{r}
# some R code for part 2b
print.default(model1$covariance.matrix)
```
```{r}
# some R code for part 2b
summary(model1)
summary(model1b)
```
The intercept estimate as shown in the print out is 4.97169 and the estimated standard error is 0.53429. For the regression coefficient the estimate is 0.79231 and the estimated standard error is 0.03906. Using a Z-test, we get:
$$P(Z \leq |z|)=2 \cdot \Phi(-|Z|), \hspace{1cm} Z=\frac{x-\mu}{\sigma} $$
In our case, the $H_0$ hypotheses is that $\mu$ is zeros, and thus we get $Z=x/sigma}$:
```{r}
cat("Z-values for the regression coefficients: ")
print.default(model1$z_value)
```
Computing the p-values using
$$P(Z \leq z) = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{z} e^{-t^2}dt$$
The compution is implemented in our mylm package, and gives the values:
```{r}
cat("P-values for the regression coefficients: ")
print.default(model1$p_value2)
```
Which suggests that both the intercept and the regression  coefficients are significant. The usual level to determen if a coefficient is siginficant or not is a 95%-confidence interval where $P(Z \leq z) < 0.05$. If true, the parameter in question is significant at a 5%-level, which both our parameters are in this case.
**c)**
```{r}
# some R code for part 2c)
library(ggplot2)
plot(model1)
plot(model1b)
```
The plot shows the residuals plotted against the fitted values. A residual plot shows if the linear regression is appropriate for the data. A random distribution around the horisontal-axis suggest that there is no systematical error in the regression, and that a linear regression is appropriate. On the other side, if the residuals follow a systematix distribution around the horisontal-axis, there is likely that the relationship between the covariates and the response is non-linear. In this plot we 
interprets the plot as randomly distributed, and that the relationship can be described as a linear regression.
**d)**
After a scaling, the χ2-distribution is the limiting distribution of an F-distribution as the denominator degrees of freedom goes to infinity. The normalization is χ2 = (numerator degrees of freedom)·F.
• What is the residual sum of squares (SSE) and the degrees of freedom for this model?
• What is total sum of squares (SST) for this model? Test the significance of the regression using a
χ2-test.
• What is the relationship between the χ2- and z-statistic in simple linear regression? Find the criticalvalue(s) for both tests.

* The residual sum of squares (SSE) for this model is computed as: $SSE = \sum_{i=1}^{n} \epsilon_i^2$ where $\epsilon_i = (I-H)Y$. The degrees of freedom for this model is the number of dimesions that are free, which can be expressed by $df = n-p-1$, where n is the number of datapoints (in this caes 3987) and p is the number of explanatory parameters (in this case 1). Thus $df = 3985$.

* The total sum of squares (SST) is $\sum_{i=1}^{n}(Y - mean(Y))^2$
